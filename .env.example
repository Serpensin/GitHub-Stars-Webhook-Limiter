# GitHub Events Limiter Configuration
# Copy this file to .env and fill in your values

# REQUIRED: Encryption key for storing webhook secrets securely
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
# DO NOT LOSE THIS KEY - you cannot decrypt secrets without it!
ENCRYPTION_KEY=your-encryption-key-here

# REQUIRED: Admin password hash for API key management panel
# Generate with: python -c "from argon2 import PasswordHasher; ph = PasswordHasher(); print(ph.hash('your-password-here'))"
# Example hash format: $argon2id$v=19$m=65540,t=3,p=4$odXOzx9oKaf783rBTMDUSl8EZNa6gwh0Giii4pWVbq0$tT4m3XF+EOMNZ4urU/gMSLPwL/HKr9c1EFXk9c1dg8k
# IMPORTANT: Do NOT add quotes around the hash - they will be included in the value
# For docker-compose.yml, use single quotes. For docker --env-file, use debug_docker.ps1 script.
ADMIN_PASSWORD_HASH=your-argon2-hash-here

# REQUIRED: Flask secret key for session management
# Generate with: python -c "import secrets; print(secrets.token_hex(32))"
FLASK_SECRET_KEY=your-flask-secret-key-here

# OPTIONAL: Sentry DSN for error monitoring and performance tracking
# Get this from your Sentry project settings
# SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id

# OPTIONAL: GitHub Personal Access Token for higher API rate limits
# Without this, you're limited to 60 API requests per hour
# With this, you get 5000 requests per hour
# Create at: https://github.com/settings/tokens (no scopes needed for public repos)
# GITHUB_TOKEN=ghp_your_github_personal_access_token

# OPTIONAL: Logging level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
LOG_LEVEL=INFO
# FLASK_DEBUG=0

# =============================================================================
# Testing Configuration (Optional)
# =============================================================================

# OPTIONAL: Auto-create a test API key on server startup
# This is useful for automated testing and CI/CD pipelines
# The key will be created with full admin permissions on first run
# and persists across database resets
#
# TEST_API_KEY_PLAINTEXT=my-static-test-api-key-12345
# TEST_API_KEY_NAME=Auto-Created Test Key

# OPTIONAL: Test repository URL for endpoint testing
# Use a real GitHub repository you have access to
# TEST_GITHUB_REPO_URL=https://github.com/owner/repo

# OPTIONAL: Valid Discord webhook URL for testing
# Get this from Discord: Server Settings -> Integrations -> Webhooks
# TEST_DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/123456789/abcdefghijklmnop

# OPTIONAL: Invalid Discord webhook for negative testing (do not change this value)
# TEST_INVALID_DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/invalid/invalid

# OPTIONAL: Base URL for the running test server
# Default: http://localhost:5000
# TEST_BASE_URL=http://localhost:5000

# =============================================================================
# Database Configuration
# =============================================================================

# OPTION 1: Use individual PostgreSQL credentials (recommended for external databases)
# The application will automatically build the DATABASE_URL from these variables.
# If these are not provided, the application will:
#   1. Check if embedded PostgreSQL is running (localhost:5432) and use internal credentials
#   2. Fall back to SQLite if no PostgreSQL is available
#
# POSTGRES_HOST=postgres.example.com
# POSTGRES_PORT=5432
# POSTGRES_DB=starlimiter
# POSTGRES_USER=starlimiter
# POSTGRES_PASSWORD=your_secure_password

# OPTION 2: Use complete DATABASE_URL (alternative to individual credentials)
# If DATABASE_URL is set, it takes priority over POSTGRES_* variables
# Leave empty or omit to use automatic detection (embedded PostgreSQL or SQLite)
# Example: DATABASE_URL=postgresql://starlimiter:mypassword@postgres.example.com:5432/starlimiter
# DATABASE_URL=

# NOTE: The :postgresql Docker image includes an embedded PostgreSQL server.
# When using docker-compose.postgresql.yml, no configuration is needed.
# The application automatically detects and uses the internal PostgreSQL server.
